{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fdd74a-f277-442b-ac3b-583ee75577ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6817af59-71c4-4f31-be73-7ba86440beee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Download & Load Dataset\n",
    "# NEU-DET dataset contains six types of defects: crazing, inclusion, patches, pitted surface, rolled-in scale, scratches.\n",
    "dataset_path = \"path_to_NEU-DET\"\n",
    "img_size = 224  # MobileNetV2 input size\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2913df1-e6d5-4d0c-b0d9-42de142ac9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Data Preprocessing & Augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,  # Normalize pixel values\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2  # 80-20 train-validation split\n",
    ")\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_generator = datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bade15d-3551-49a2-850c-e396483ba363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Load Pretrained MobileNetV2 & Modify for Our Dataset\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(img_size, img_size, 3))\n",
    "base_model.trainable = False  # Freeze the base model\n",
    "\n",
    "# Add custom classification head\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dense(len(train_generator.class_indices), activation='softmax')(x)  # Output layer\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afb0dbe-7d4c-4398-9a5c-95050edaed0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Train the Model\n",
    "epochs = 10\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=epochs,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b29b082-8be6-447c-82d1-d4b300c4e38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Evaluate the Model\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda449a4-3ca6-42d3-8740-db7e355b130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Visualizing Feature Importance with Grad-CAM\n",
    "def get_gradcam(img_path, model, last_conv_layer_name):\n",
    "    \"\"\"\n",
    "    Generate a Grad-CAM heatmap for a given image.\n",
    "    \"\"\"\n",
    "    img = tf.keras.preprocessing.image.load_img(img_path, target_size=(img_size, img_size))\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(img)/255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "    # Create a model that maps input image to last conv layer output & predictions\n",
    "    grad_model = tf.keras.models.Model([\n",
    "        model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "    )\n",
    "    \n",
    "    # Compute gradients\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_outputs, predictions = grad_model(img_array)\n",
    "        class_idx = np.argmax(predictions[0])\n",
    "        loss = predictions[:, class_idx]\n",
    "    \n",
    "    grads = tape.gradient(loss, conv_outputs)\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "    conv_outputs = conv_outputs[0] * pooled_grads\n",
    "    heatmap = np.mean(conv_outputs, axis=-1)\n",
    "    heatmap = np.maximum(heatmap, 0) / np.max(heatmap)\n",
    "    heatmap = cv2.resize(heatmap, (img_size, img_size))\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    return heatmap\n",
    "\n",
    "# Example usage of Grad-CAM\n",
    "sample_image_path = \"path_to_sample_image.jpg\"\n",
    "gradcam_result = get_gradcam(sample_image_path, model, \"Conv_1\")\n",
    "plt.imshow(gradcam_result, cmap='jet')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
